{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "skLzwOSTG8oc",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import kurtosis, skew\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import statistics as st\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "import random\n",
        "from scipy.fft import fft\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#feature selection variance based\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import matplotlib.pyplot as plt\n",
        "#feature selection tree based\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from tensorflow.python import keras\n",
        "import tensorflow as tf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JrhHC3g6G8oe",
        "outputId": "9f861bf8-c7d7-466d-a0a1-897edfec7d40",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script>Jupyter.notebook.kernel.restart()</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.core.display import HTML\n",
        "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets_path = \"CI_Dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjbAHdAIHBcF",
        "outputId": "59c0b181-8aa3-4d61-e968-8e16e5649a45"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "datasets_path = \"/content/drive/MyDrive/CI_Dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XTOhQPOuG8of",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "seed = 57\n",
        "\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bqiluHWwG8of",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#reading 2 channels of the dataset\n",
        "data_points_channel_1 = pickle.load(open(datasets_path + '/data_points_channel_1.pkl', 'rb'))\n",
        "data_points_channel_2 = pickle.load(open(datasets_path + '/data_points_channel_2.pkl', 'rb'))\n",
        "labels = pickle.load(open(datasets_path + '/labels.pkl', 'rb'))\n",
        "\n",
        "sampling_freq = 256\n",
        "\n",
        "\n",
        "b, a = butter(3, [0.5, 40], btype='bandpass', fs=sampling_freq)\n",
        "\n",
        "data_points_channel_1_filtered = np.array(\n",
        "    [lfilter(b, a, data_points_channel_1[ind, :]) for ind in range(data_points_channel_1.shape[0])])\n",
        "data_points_channel_2_filtered = np.array(\n",
        "    [lfilter(b, a, data_points_channel_2[ind, :]) for ind in range(data_points_channel_2.shape[0])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k1jokw8uG8og",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def featured(x1):\n",
        "    result = np.zeros((x1.shape[0], 15))\n",
        "    for i in range(len(x1)):\n",
        "        #statistical features\n",
        "        result[i][0] = np.mean(x1[i])\n",
        "        result[i][1] = np.var(x1[i])\n",
        "        result[i][2] = np.std(x1[i])\n",
        "        result[i][3] = skew(x1[i])\n",
        "        result[i][4] = kurtosis(x1[i])\n",
        "        result[i][5] = np.max(x1[i])\n",
        "        result[i][6] = np.min(x1[i])\n",
        "        result[i][7] = np.median(x1[i])\n",
        "        result[i][8] = st.mode(x1[i])\n",
        "        #time domain features\n",
        "        result[i][9] = np.ptp(x1[i])\n",
        "        result[i][10] = np.mean(np.absolute(x1[i] - np.mean(x1[i])))\n",
        "        #frequency domain features\n",
        "        ft = fft(x1[i])\n",
        "        s = np.abs(ft ** 2) / x1.shape[0]\n",
        "        result[i][11] = np.max(s)\n",
        "        result[i][12] = np.sum(s)\n",
        "        result[i][13] = np.mean(s)\n",
        "        result[i][14] = np.var(s)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YWrPx7wJG8og",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def normalzie(data):\n",
        "    x_normalized = np.zeros_like(data)\n",
        "    scaler = MinMaxScaler()\n",
        "    for i in range(data.shape[1]):\n",
        "        x_normalized[:, i] = scaler.fit_transform(data[:, i].reshape(-1, 1)).flatten()\n",
        "    return x_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EhVxTZMhG8og",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "featured_data_ch1 = featured(data_points_channel_1_filtered)\n",
        "featured_data_ch2 = featured(data_points_channel_2_filtered)\n",
        "normal_featured_data_ch1 = normalzie(featured_data_ch1)\n",
        "normal_featured_data_ch2 = normalzie(featured_data_ch2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NjeHpyffG8og",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "normal_data_ch1 = normalzie(data_points_channel_1_filtered)\n",
        "normal_data_ch2 = normalzie(data_points_channel_2_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y7dwWscsG8oh",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data_raw = []\n",
        "data_raw.append([])\n",
        "data_raw.append([])\n",
        "\n",
        "data_raw[0] = normal_data_ch1\n",
        "data_raw[1] = normal_data_ch2\n",
        "\n",
        "#adding two channels into one\n",
        "data_raw = np.array(data_raw)\n",
        "features_concat = np.concatenate((normal_featured_data_ch1, normal_featured_data_ch2), axis=0)\n",
        "labels_cat = np.concatenate((labels, labels), axis=0)\n",
        "target = data_raw.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_4lESpl2G8oh",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#selecting features\n",
        "clf = ExtraTreesClassifier(n_estimators=50)\n",
        "clf = clf.fit(features_concat, labels_cat)\n",
        "clf.feature_importances_\n",
        "\n",
        "model = SelectFromModel(clf, prefit=True)\n",
        "data_featured = model.transform(features_concat)\n",
        "\n",
        "feature_idx = model.get_support()\n",
        "feature_indices = [index for index in range(len(feature_idx)) if feature_idx[index] == 1]\n",
        "\n",
        "data_featured = np.concatenate((data_featured[:target], data_featured[target:]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cKYBYy3VG8oj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "x, y, z = data_raw.shape\n",
        "data_raw_new = np.zeros((y, z, x))\n",
        "\n",
        "for i in range(y):\n",
        "    for j in range(z):\n",
        "        for k in range(x):\n",
        "            data_raw_new[i][j][k] = data_raw[k][i][j]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7xjqh-d9G8oj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# making labels based on the categorical cross entropy\n",
        "categorical_label=np.zeros((len(labels),2),int)\n",
        "for i in range(labels.shape[0]):\n",
        "    if labels[i]==0:\n",
        "        categorical_label[i]=np.array((0,1),int)\n",
        "    if labels[i]==1:\n",
        "        categorical_label[i]=np.array((1,0),int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NIvhJae-G8oj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def data_split(data, label, feature):\n",
        "    indices = np.arange(target)\n",
        "    indices_train, indices_test = train_test_split(indices, test_size=0.2, random_state=seed)\n",
        "\n",
        "    data_test = data[indices_test]\n",
        "    label_test = label[indices_test]\n",
        "    feature_test = feature[indices_test]\n",
        "\n",
        "    indices_train_train, indices_train_val = train_test_split(indices_train, test_size=0.25, random_state=seed)\n",
        "\n",
        "    data_train = data[indices_train_train]\n",
        "    label_train = label[indices_train_train]\n",
        "    feature_train = feature[indices_train_train]\n",
        "\n",
        "    data_val = data[indices_train_val]\n",
        "    label_val = label[indices_train_val]\n",
        "    feature_val = feature[indices_train_val]\n",
        "\n",
        "    return data_train, label_train, feature_train, data_test, label_test, feature_test, data_val, label_val, feature_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SxX07n76G8oj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "result = data_split(data_raw_new, categorical_label, data_featured)\n",
        "data_points_train, labels_train, features_train, data_points_test, labels_test, features_test, data_points_val, labels_val, features_val = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DxGkoOgAG8oj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86t3mWw3G8ok",
        "outputId": "46ecfc91-b6f6-4f68-b138-61e7fa8a697a",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Test Seizure //loading test dataset // normalizing // extracting features // feature selection\n",
        "\n",
        "test_seizure_data_points_channel_1 = pickle.load(open(datasets_path + '/seizure_test_data_points_ch1.pkl', 'rb'))\n",
        "test_seizure_data_points_channel_2 = pickle.load(open(datasets_path + '/seizure_test_data_points_ch2.pkl', 'rb'))\n",
        "\n",
        "test_seizure_labels = np.ones(test_seizure_data_points_channel_1.shape[0])\n",
        "\n",
        "sampling_freq = 256\n",
        "#based on info from website\n",
        "\n",
        "b, a = butter(3, [0.5, 40], btype='bandpass', fs=sampling_freq)\n",
        "\n",
        "test_seizure_data_points_channel_1_filtered = np.array(\n",
        "    [lfilter(b, a, test_seizure_data_points_channel_1[ind, :]) for ind in range(test_seizure_data_points_channel_1.shape[0])])\n",
        "test_seizure_data_points_channel_2_filtered = np.array(\n",
        "    [lfilter(b, a, test_seizure_data_points_channel_2[ind, :]) for ind in range(test_seizure_data_points_channel_2.shape[0])])\n",
        "\n",
        "featured_test_seizure_data_ch1 = featured(test_seizure_data_points_channel_1_filtered)\n",
        "featured_test_seizure_data_ch2 = featured(test_seizure_data_points_channel_2_filtered)\n",
        "normal_featured_test_seizure_data_ch1 = normalzie(featured_test_seizure_data_ch1)\n",
        "normal_featured_test_seizure_data_ch2 = normalzie(featured_test_seizure_data_ch2)\n",
        "\n",
        "normal_test_seizure_data_ch1 = normalzie(test_seizure_data_points_channel_1_filtered)\n",
        "normal_test_seizure_data_ch2 = normalzie(test_seizure_data_points_channel_2_filtered)\n",
        "\n",
        "test_seizure_data_raw = []\n",
        "test_seizure_data_raw.append([])\n",
        "test_seizure_data_raw.append([])\n",
        "\n",
        "test_seizure_data_raw[0] = normal_test_seizure_data_ch1\n",
        "test_seizure_data_raw[1] = normal_test_seizure_data_ch2\n",
        "\n",
        "test_seizure_data_raw = np.array(test_seizure_data_raw)\n",
        "\n",
        "x, y, z = test_seizure_data_raw.shape\n",
        "test_seizure_data_raw_new = np.zeros((y, z, x))\n",
        "\n",
        "for i in range(y):\n",
        "    for j in range(z):\n",
        "        for k in range(x):\n",
        "            test_seizure_data_raw_new[i][j][k] = test_seizure_data_raw[k][i][j]\n",
        "\n",
        "\n",
        "test_seizure_features_concat = np.concatenate((featured_test_seizure_data_ch1, featured_test_seizure_data_ch2), axis=0)\n",
        "test_seizure_labels_cat = np.concatenate((test_seizure_labels, test_seizure_labels), axis=0)\n",
        "\n",
        "test_seizure_features = test_seizure_features_concat[:, feature_indices]\n",
        "\n",
        "x = test_seizure_features.shape[0] // 2\n",
        "\n",
        "test_seizure_features = np.concatenate((test_seizure_features[:x], test_seizure_features[x:]), axis=1)\n",
        "\n",
        "categorical_test_seizure_label=np.zeros((len(test_seizure_labels),2),int)\n",
        "for i in range(test_seizure_labels.shape[0]):\n",
        "    if test_seizure_labels[i]==0:\n",
        "        categorical_test_seizure_label[i]=np.array((0,1),int)\n",
        "    if test_seizure_labels[i]==1:\n",
        "        categorical_test_seizure_label[i]=np.array((1,0),int)\n",
        "\n",
        "test_seizure_labels = categorical_test_seizure_label\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pxPSM-EwG8ok",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "test_plot=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NZbj9wm5G8ok",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def cnn(filter1,filter2,kernel1,kernel2,dense1,dense2):\n",
        "    input1 = tf.keras.Input(shape=data_raw_new[0].shape, name=\"first datas\")\n",
        "    input2 = tf.keras.Input(shape=data_featured[0].shape, name=\"featured datas\")\n",
        "\n",
        "    model1 = tf.keras.layers.Conv1D(filters=filter1, kernel_size=kernel1,strides=2, activation='relu',name='convolution_layer1')(input1)\n",
        "    model1 = tf.keras.layers.MaxPool1D(pool_size=2,name='pooling_layer1')(model1)\n",
        "\n",
        "\n",
        "    model1 = tf.keras.layers.Conv1D(filters=filter2, kernel_size=kernel2,strides=2, activation='relu',name='convolution_layer2')(model1)\n",
        "    model1 = tf.keras.layers.MaxPool1D(pool_size=2,name='pooling_layer2')(model1)\n",
        "\n",
        "\n",
        "    model1 = tf.keras.layers.Flatten(name='flatten_layer')(model1)\n",
        "\n",
        "    model1 = tf.keras.layers.Dense(dense1, activation=\"relu\",name='Dense_layer1')(model1)\n",
        "\n",
        "\n",
        "    model1 = tf.keras.layers.Dense(dense2, activation=\"relu\",name='Dense_layer_cnnfeatures')(model1)\n",
        "\n",
        "\n",
        "    model2 = tf.keras.layers.Dense(data_featured.shape[1], activation=\"relu\",name='Dense_layer_selectedfeature', trainable=False)(input2)\n",
        "\n",
        "    concat = tf.keras.layers.Concatenate()([model1, model2])\n",
        "\n",
        "    result = tf.keras.layers.Dense(2, activation=\"softmax\",name='output_layer')(concat)\n",
        "\n",
        "    model_final = tf.keras.Model([input1, input2], result)\n",
        "\n",
        "    model_final.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                        loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"acc\",f1_m,precision_m, recall_m])\n",
        "    res = model_final.fit([data_points_train, features_train], y=labels_train, batch_size=20, epochs=8,\n",
        "                          validation_data=([data_points_val, features_val], labels_val))\n",
        "\n",
        "    accuracy=res.history['acc']\n",
        "    value_accuracy=res.history['val_acc']\n",
        "    loss=res.history['loss']\n",
        "    value_loss=res.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(accuracy, label='Training Accuracy')\n",
        "    plt.plot(value_accuracy, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()), 1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(value_loss, label='Validation Loss')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    false_alarm=model_final.evaluate(x=[test_seizure_data_raw_new,test_seizure_features],y=test_seizure_labels,verbose=1)\n",
        "    test_result= model_final.evaluate(x=[data_points_test,features_test],y=labels_test,verbose=0)\n",
        "\n",
        "    print(test_result)\n",
        "    print(false_alarm)\n",
        "\n",
        "    y_prediction=model_final.predict([test_seizure_data_raw_new,test_seizure_features])\n",
        "    y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "    y_test=np.argmax(test_seizure_labels, axis=1)\n",
        "\n",
        "    #for test data\n",
        "    y_prediction=model_final.predict([data_points_test,features_test])\n",
        "    #Predict\n",
        "\n",
        "    y_prediction = np.argmax (y_prediction, axis = 1)\n",
        "    y_test=np.argmax(labels_test, axis=1)\n",
        "    #Create confusion matrix and normalizes it over predicted (columns)\n",
        "    result_confusion = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
        "    print(result_confusion)\n",
        "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = result_confusion, display_labels = [False, True])\n",
        "    cm_display.plot()\n",
        "    plt.show()\n",
        "    test_plot.append(test_result[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PE0dVqsfG8ok",
        "outputId": "f38bb8ab-bfbe-4cc1-a4cb-c02061c04531",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "cnn(60,60,4,4,100,20)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
